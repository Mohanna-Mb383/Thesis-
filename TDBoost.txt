
 appendix A:


 library(lattice)
 library(TDboost)
 library(coda)
 library(Matrix)
 library(splines)
 library(cplm)
 data(AutoClaim)
 AutoClaim
 head(AutoClaim)
 AutoClaim<- as.data.frame(AutoClaim)


 da <- subset(AutoClaim, IN_YY == 1) # use data in the Yip and Yau paper
 da <- transform(da, CLM_AMT5 = CLM_AMT5/1000,INCOME = INCOME/10000)
 summary(da$CLM_AMT5)
 sum(da$CLM_AMT5 == 0)/nrow(da)
 fit <- TDboost(CLM_AMT5 ~ AGE+BLUEBOOK+ HOMEKIDS+
 MARRIED +KIDSDRIV+MVR_PTS+NPOLICY+
 TRAVTIME+ AREA+CAR_USE+CAR_TYPE+GENDER+
 JOBCLASS+MAX_EDUC+REVOLKED,data = da,
 distribution=list(name="EDM",alpha=1.5),
 # specify Tweedie index parameter
 n.trees=3000 ,
 shrinkage=0.005,
 # 0.001 to 0.1 usually work
 interaction.depth=3,
 bag.fraction = 0.5,
 train.fraction = 0.5,
 # number of trees
 # shrinkage or learning rate,
 # 1: additive model, 2: two-way interactions, etc.
 # subsampling fraction, 0.5 is probably best
 # fraction of data for training,
 # first train.fraction*N used for training
 n.minobsinnode = 10,
 # minimum total weight needed in each node
 cv.folds = 5,
 keep.data=TRUE,
 verbose=TRUE)
 # do 5-fold cross-validation
 # keep a copy of the dataset with the object
 # print out progress
 #print out the optimal iteration number M
 best.iter <- TDboost.perf(fit,method="test")
print(best.iter)
 summary(TDboost.perf(fit,method="test"))
 # check performance using 5-fold cross-validation
 best.iter <- TDboost.perf(fit,method="cv")
 print(best.iter)
 summary(TDboost.perf(fit,method="cv"))
 # plot the performance
 # plot variable influence
 summary(fit,n.trees=1)
 # based on the first tree
 summary(fit,n.trees=best.iter) # based on the estimated best number of trees
 # making prediction on data2
 f.predict <- predict.TDboost(fit, AutoClaim, best.iter) #model prediction / scoring
 print(sum((AutoClaim$y- f.predict)^2))
 print(sum((AutoClaim$ CLM_AMT5- f.predict)^2))
 #least squares
 # plot loss function as a result of n trees added to the ensemble
 TDboost.perf(fit, method = "cv")
 TDboost.perf(fit, plot.it = TRUE,
 oobag.curve = FALSE, overlay = TRUE, "cv")
 par(mar = c(5, 8, 1, 1))
 summary(fit,cBars = 16,method = relative.influence)
 summary(fit,method = relative.influence)
 #find index for n trees with minimum CV error
 min_MSE <- which.min(fit$cv.error)
 # get MSE and compute RMSE
 sqrt(fit$cv.error[min_MSE])
 # create marginal plots
 # plot variable 1 after "best" iterations
 plot.TDboost(fit,1,best.iter)
 plot(fit, i.var = 1:3, n.trees = best.iter)
 predict=predict.TDboost(fit, AutoClaim, best.iter,
 single.tree=FALSE,type=c("response","link"))
predmatrix<-predict(fit,AutoClaim,n.trees = n.trees)
 # do another 20 iterations
 TDboost2 <- TDboost.more(fit,20,verbose=FALSE) # stop printing detailed progress
 # fit a gamma model (when alpha = 2.0)
 data2 <- da[da$CLM_AMT5 !=0,]
 TDboost3 <- TDboost(CLM_AMT5 ~AGE+BLUEBOOK+ HOMEKIDS
 + MARRIED +KIDSDRIV + MVR_PTS+NPOLICY+TRAVTIME
 + AREA+CAR_USE+CAR_TYPE+GENDER+JOBCLASS
 +MAX_EDUC+MARRIED+REVOLKED,data = da,
 distribution=list(name="EDM",alpha=2),
 # specify Tweedie index parameter
 n.trees=3000 ,
 # number of trees
 shrinkage=0.005,
 # 0.001 to 0.1 usually work
 interaction.depth=3,
 bag.fraction = 0.5,
 train.fraction = 0.5,
 # shrinkage or learning rate,
 # 1: additive model, 2: two-way interactions, etc.
 # subsampling fraction, 0.5 is probably best
 # fraction of data for training,
 # first train.fraction*N used for training
 n.minobsinnode = 10,
 # minimum total weight needed in each node
 cv.folds = 5,
 keep.data=TRUE,
 verbose=TRUE)
 # do 5-fold cross-validation
 # keep a copy of the dataset with the object
 # print out progress
 best.iter <- TDboost.perf(TDboost3,method="test")
 c(coef(fit), p = fit$p, phi = fit$phi)
 print(best.iter)
 summary(TDboost.perf(TDboost3,method="test"))
 # check performance using 5-fold cross-validation
 best.iter <- TDboost.perf(TDboost3,method="cv")
 print(best.iter)
 summary(TDboost.perf(TDboost3,method="cv"))
 P1 <- TDboost(CLM_AMT ~ CAR_USE + REVOLKED+AREA+
 MARRIED +CAR_TYPE , distribution=list(name="EDM",alpha=1.5) ,data = da)
 P2 <- TDboost(CLM_AMT ~ factor(CAR_USE) + factor(REVOLKED) +
 factor(GENDER) + factor(AREA) +factor(MARRIED) +
 factor(CAR_TYPE), distribution=list(name="EDM",alpha=1.5)
,data =da)
 P3 <- TDboost(CLM_AMT ~ factor(CAR_USE) + factor(REVOLKED) +
 factor(GENDER) +factor(AREA) +
 factor(MARRIED) + factor(CAR_TYPE) +
 TRAVTIME + MVR_PTS + INCOME,
 distribution=list(name="EDM",alpha=1.5) ,
 data = da)
 # compute the Gini indices
 gg <- gini(loss = "CLM_AMT", score = paste("P", 1:3, sep = ""),
 data = da)
 gg
 # plot the Lorenz curves theme_set(theme_bw())
 plot(gg)
 plot(gg, overlay = FALSE)